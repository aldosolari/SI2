---
title: "**Hypothesis testing**"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval=T, 
                      message=F, 
                      warning=F, 
                      error=F, 
                      comment=NA, 
                      cache=F, 
                      R.options=list(width=220))
```

# Outline


* Darwin data

* Lindley's paradox

* Confident conclusions

---

# Darwin data

* Charles Darwin collected data over a period of years on the heights of Zea mays plants

* The plants were descended from the same parents and planted at the same time. 

* Half of the plants were *self-fertilized*, and half were
*cross-fertilized*, and the purpose of the experiment was to compare their *heights*. 

* In order to minimize differences in humidity, growing conditions, and lighting, Darwin planted the seeds in pairs in different pots

---

```{r}
library(SMPracticals)
data(darwin)
darwin_pair = data.frame(Pot = darwin[darwin$type=="Cross",1],
           Cross = darwin[darwin$type=="Cross",4],
           Self = darwin[darwin$type=="Self",4])
knitr::kable(darwin_pair, "html")
```

---

# Questions

1. What is the effect of cross-fertilization? Are cross-fertilized plants equal, smaller or taller than self-fertilized plants?

2. Which statistical model would you use to describe the experiment?

3. Formulate a null hypothesis about the parameter of interest, and perform a test. What is your conclusion?

---

```{r}
library(ggpubr)
 p <- ggboxplot(darwin, x = "type", y = "height",
                color = "type", add = "jitter")
 p
```

---

```{r}
t.test(height ~ type, data=darwin, var.equal=TRUE)
```

---

```{r}
ggline(darwin, x = "type", y = "height", group="pair")
```

---

```{r}
differences = apply(darwin_pair[,3:2],1,diff)
t.test(differences)
```

---

# Point null hypothesis

* Testing a point null hypothesis $H_0: \theta=0$ against $H_1:\theta \neq 0$ is a classical but controversial issue in statistical methodology

* Statisticians have classically asked the wrong question 

> Is the effect 0? -- but the effect is always different from 0 -- at least in some remote decimal place 

* The question that should be addressed instead is 

> Is the effect large enough to be meaningfully different from 0?

---

# ESP experiment

* We give an example from parapsychological research 

* The case at hand involved the test
of a subjectâ€™s claim to affect a series of randomly generated zeros and ones by means of extrasensory capacities

* The subject claimed
that his ESP would make the sample mean differ significantly from $1/2$

---

# Data

* $n=104490000$

* $Y_1,\ldots, Y_n$ i.i.d. Bernoulli $(\theta)$

* $H_0: \theta=1/2$ against $H_{1}: \theta \neq 1/2$

* Observed $\sum_{i=1}^{n}y_i = 52263471$ ones, $n-\sum_{i=1}^{n}y_i = 52226529$ zeros

```{r}
binom.test(52263471, 104490000, p = 0.5)
```

---

# Equivalence and non-equivalence

*  $X\sim N(\theta,1)$ and $\theta \in \mathbb{R}$

*  Pre-specify a margin $\Delta \geq 0$ 

* Equivalence region $E=[-\Delta,\Delta]$ 

* Non-equivalence region $\bar{E} = (- \infty,-\Delta) \cup (\Delta, +\infty)$

* Hypotheses $H_E: \theta \in E$ and $H_{\bar{E}}: \theta \in \bar{E}$

---

# Asymmetry in conclusions

* Testing one hypothesis (the null) against the other (the alternative) at level $\alpha=0.05$ leads to the well-known asymmetry in conclusions

* Rejecting the null implies the conclusion that the true parameter $\theta$ belongs to the region specified by the alternative, which is the correct conclusion $95\%$ of the times

* Not rejecting the null does not allow to conclude that $\theta$ is in the region specified by the null: it leaves us with uncertainty about where $\theta$ lies

---

# Back to symmetry

* Symmetry can be restored by testing again but interchanging the null and the alternative.

* This involves simultaneously testing the two null hypotheses $H_E$ and $H_{\bar{E}}$ at level $\alpha$

* For instance, even if the experiment was exclusively planned for concluding equivalence, it would be still possible to conclude non-equivalence for free


* The conclusion is then that the observed data $x$ is consistent with $E$, with $\bar{E}$, or with both, i.e. complete uncertainty


---

# Inference on the standardized effect


* Let $Y_1,\ldots,Y_n$ be a random sample from $N(\mu,\sigma^2)$

* Suppose that the parameter of interest is the standardized effect

$$
 \theta = \frac{\mu}{\sigma}
$$


* Older by far than any other statistical technique is *point estimation*, i.e.
$$
 \hat{\theta} = \frac{\bar{Y}}{S}
$$
where $\bar{X}$ is the sample mean and $S^2$ is the unbiased estimator of the variance based on a random sample of size $n$

* A point estimate gives a hint of the most likely value the data seem to point out or suggest, but by itself this is not enough because it ignores variability and gives no insight into uncertainty


---

# Confident conclusions

* Tukey advocated that the answer should be provided by *confident conclusions*

* Confident conclusions are of two types: *directions* and *intervals*

* Confidence intervals address the original question *What are the likely values for the effect?* by pointing out a whole interval of possible values, chosen so that there can be high confidence that the true value of the effect is among them

* Confidence directions address a simpler question: *What is the sign of the effect?* Is it *positive*, *negative* or *irrelevant*?

* Since asking only for direction asks for a cruder form of information, we may expect to have more statistical power to answer the question

---

# Relevance boundary

* Small effects usually matter very little, i.e. are not considered of practical importance

* It is the researcher's duty to have enough subject matter knowledge in order to know the magnitude of a minimally relevant effect

* The researcher can choose a relevance boundary $\Delta$ such that an effect is considered 
    - *irrelevant* when $-\Delta \leq \theta \leq \Delta$

    - *positive* when $\theta>\Delta$ 

    - *negative* when $\theta< -\Delta$.

---

# Confidence interval

* Lower bound $\underline{\theta}_{\alpha}$  such that $$\displaystyle T_{n-1,\frac{\underline{\theta}_{\alpha}}{\sqrt{n}}}(\sqrt{n}\hat{\theta}_{\mathrm{obs}}) = 1-\alpha$$ 

* Upper bound $\overline{\theta}_{\alpha}$ such that $\displaystyle T_{n-1,\frac{\overline{\theta}_{\alpha}}{\sqrt{n}}}(\sqrt{n}\hat{\theta}_{\mathrm{obs}}) = \alpha$



---

# Confidence direction


$$D^{1-\alpha}  = \left\{
    \begin{array}{lll}
          (\Delta, \infty) & \textrm{(positive)} & \mathrm{if\,\,} \hat{\theta} >c/ \sqrt{n} \\
          (-\infty, -\Delta) & \textrm{(negative)} & \mathrm{if\,\,}   \hat{\theta} < - c/ \sqrt{n} \\
   \left[ - \Delta, \Delta \right] & \textrm{(irrelevant)} & \mathrm{if\,\,} -k/ \sqrt{n}   < \hat{\theta} < k/ \sqrt{n} \\
   (-\infty,\infty) & \textrm{(uncertain)} & \textrm{otherwise}
  \end{array}
    \right.$$

where

* $c^2 = F^{-1}_{1,n-1,n\Delta^2}(1-\tilde{\alpha})$ 

* $k^2 = F^{-1}_{1,n-1,n\Delta^2}(\tilde{\alpha})$ 

* $\tilde{\alpha}$ is such that 

$$\sup_{\theta} \mathrm{P}_{\theta}(\theta \notin D^{1-\alpha}) =  \tilde{\alpha} + T_{n-1,\sqrt{n}\Delta}(-c) = \alpha$$

---

```{r}
rm(list=ls())
alpha = 0.05
ns = 2:100
delta = 1/5
a = sapply(ns, function(n)
uniroot(function(a) {
 - alpha + a + pt(-sqrt(qf(1-a,df1=1,df2=n-1,ncp=n*delta^2)), df=n-1, ncp=sqrt(n)*delta)
  }, lower=0.01, upper = 1
)$root
)
c2 = qf(1-a,df1=1,df2=ns-1,ncp=ns*delta^2)
k2 = qf(a,df1=1,df2=ns-1,ncp=ns*delta^2)
plot(ns,sqrt(c2/ns), type="l", ylim = c(-1,1), ylab="Point estimate", xlab="n")
lines(ns,-sqrt(c2/ns))
lines(ns,sqrt(k2/ns))
lines(ns,-sqrt(k2/ns))
abline(h=delta, lty=3)
abline(h=-delta, lty=3)
segments(x0=50,x1=50, y0=0.1662, y1=0.7496)
points(50,0.46, pch=20)
```


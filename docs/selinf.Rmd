---
title: "Multiple testing and selective inference"
description: |
  Statistical Inference II
author:
  - name: Aldo Solari
    url: https://aldosolari.github.io/
    affiliation: DEMS
    affiliation_url: http://dems.unimib.it/
date: "`r Sys.Date()`"
output:
  radix::radix_article:
    toc: true
    toc_depth: 1
bibliography: selinf.bib
---

# Replicability crisis

**Reference**: @patil2016statistical


It is not difficult to find stories of a crisis in modern science, either in the popular press or in the scientific literature.

```{r, echo=FALSE}
library("htmltools")
library("vembedr")
embed_url("https://www.youtube.com/watch?v=0Rnq1NpHdmw")
```

<aside>
Scientific Studies: Last Week Tonight with John Oliver (HBO)
</aside>

*Scientific studies* should be *reproducible* and *replicable*.
Here we provide informal definitions for key scientific terms and provide detailed statistical definitions. 

**A scientific study**: consists of document(s) specifying a population, question, hypothesis,
experimental design, experimenter, data, analysis plan, analyst, code, parameter estimates, and
claims about the parameter estimates.

**Reproducibility** is defined as reperforming
the same analysis with the same code using a
different analyst.

```{r}
library(scifigure)
reproduce_figure()
```

**Replicability** is defined as reperforming
the experiment and collecting new
data.

```{r}
replicate_figure()
```



**Strongly replicable study**: Given a population, hypothesis, experimental design, analysis plan,
and code you get consistent estimates when you recollect data and perform the analysis using
the original code. In other words, the only aspect that varies from study 1 to study 2 is the resampling
of the population.

**Replicable study**: Given a population, hypothesis, experimental design, and analysis plan you
get consistent estimates when you recollect data and redo the analysis.
The distinction between this and strong
replication is that we are resampling from the population but are allowing the new study to
analyze the data in their own way.

**Publication**: Making a public claim on the basis of a scientific study.

**Strongly replicable claim**: Given a population, hypothesis, experimental design, analysis plan,
and code, you make an equivalent claim based on the results of the study.

**Replicable claim**: Given a population, hypothesis, experimental design, and analysis plan, you
make an equivalent claim based on the results of the study.

**False discovery**: The claim at the conclusion of a scientific study is not equal to the claim you
would make if you could observe all data from the population given your hypothesis,
experimental design, and analysis plan.

**Garden of forking paths**: Given a population, hypothesis, experimental design, experimenter,
data, analysis plan, and analyst, the code changes given the data you observe.

**P-hacking**:
Given a population, hypothesis, experimental design, experimenter, data, analysis
plan, and analyst, the code changes to match a desired statemen

**File drawer effect**: The probability of publication depends on the claim made at the conclusion
of a scientific study


### Toy example: male height study

* $P$: All male humans on Earth at current time
* $Q$: What is the size of the average male?
* $H$: The average male is 174 cm.
* $ED$: Gather a sample of $n$ accessible males and use a tape measure to measure each man’s height.
* $D$: The measured heights of $n$ males in our sample.
* $AP$: Sum all of the heights and divide by $n$. Conduct a one-sample t-test and compute a $p$-value
for the test.
* $C$: A function that computes the mean of a set of values, the t-statistic, and the associated p-value.
* $\theta$ : The average height of all male humans on Earth at current time.
* $\theta^{*}$ : The average height of all accessible male humans.
* $\hat{\theta}$: The average height of our sample (with $p$-value).

* Reproduction: We would provide the data $D$ and code $C$ to a new analyst, $A_2$, and ask them to confirm the average height we calculated.

* Strong Replication of a Study: We would recruit a new experimenter $E_2$ and a new analyst
$A_2$ and describe the steps we took to measure average height in males. $E_2$ would gather a new
sample of $n$ males and $A_2$ would compute the average exactly as we did in our study. Given
assumptions of the distribution of male heights in the population, we would compare the two
averages, $\hat{\theta}_1$ from our original study and $\hat{\theta}_2$ from the study conducted by $E_2$ and $A_2$.

* Replication of a Study: A new experimenter and analyst, $E_2$ and $A_2$ are recruited, as in the
strong replication example. $E_2$ must gather a sample of $n$ males just as we did in our original
study. $A_2$ may analyze the data as they see fit. Changes from our original procedure may
include computation of a trimmed mean or median, removal of outlying observations, or other
such statistical considerations

* Strong Replication of a Claim: Suppose that our original claim $S(\hat{\theta}_1)$ is “The average male
is smaller than 174 cm”. After the strong replication version of the study is conducted, claim $S(\hat{\theta}_2)$ must be the
same.

* Replication of a Claim: Same as above, except after the standard replication.

* False Positive: If the truth, $S(\theta)$, is that “The average male is 174 cm” then our claim $S(\hat{\theta}_1)$ would not agree with $S(\theta)$. 

* p-hacking: Suppose we truly desire to state that the average male height is larger than 174 cm. We could
rewrite our code to continually manipulate the data (drop observations, transform observations,
use different statistical tests) until we are able to make this claim with statistical significance.
Alternatively, we may simply desire to report the most significant result we can find without
any specific average height in mind.

* Garden of Forking Paths: Suppose we do not fix our assumptions and analysis plan before
we observe our data, and based on the distribution of our sample we choose to run a nonparametric test instead of a t-test. If we were to take another sample that appeared normally
distributed, we may choose to apply a t-test and get different results

* File-drawer problem: If our statistical test does not produce a significant p-value, we will
disregard our study and move on to a new one that has a better hope for a significant result.

# A new scientific paradigm

There are likely multiple sources for this crisis in modern science.
One source of this crisis is the misuse of statistical methods in science, with the $p$-value receiving most of the criticism. 

It could be argued that this misuse of statistical methods is caused by a shift in how data is used in 21st century science compared to its use in the mid-20th century which presumed scientists had formal statistical hypotheses before collecting data. This paradigm has been shifted to one in which scientists *collect data first and ask questions later*.

**Textbook practice**

1. Select hypotheses/model/questions

2. Collect data

3. Perform inference

**Modern practice**

1. Collect data

2. Select hypotheses/model/questions

3. Perform inference

Elementary statistical textbooks and researchers often ignore selection issues. As a consequence, inference mat be completely wrong and misleading.

These are the problems that the area of *selective inference* attempts to address. 

### Confidence intervals for selected parameters

Often researchers examine many parameters $\theta_1,\ldots,\theta_m$ at once and report confidence intervals for *selected ones*. 

Note that if confidence intervals (CIs) have the right coverage, i.e. $\mathbb{E}(I\{\mathrm{CI}_i \ni \theta_i\}) = 95\%$ for $i=1,\ldots,m$, then the number of covering CIs divided by the number of parameters is expected to be

$$\mathbb{E}\Big(\frac{1}{m}\sum_{i=1}^{m}I\{\mathrm{CI}_i \ni \theta_i \}\Big)= 95\%$$

However, as noted by @soric1989

> In a large number of 95% confidence intervals, 95% of them contain the population parameter [...] but it would be wrong to imagine that the same rule also applies to a large number of 95% interesting confidence intervals

Suppose we have $m=20$ parameters and we construct 90% confidence intervals. 

```{r, echo=FALSE}
set.seed(12)
alpha = 0.1
m = 20
theta = rnorm(m,0,sqrt(0.04))
stats = rnorm(m,theta,1)
CIlow = stats - qnorm(alpha, lower.tail = F) 
CIup = stats + qnorm(alpha, lower.tail = F) 
plot(1:m,theta,ylim=c(-3,3), ylab=expression(theta), xlab="i")
abline(h=0)
select = (CIlow*CIup>0)
for (i in 1:m){
  segments(x0=i,x1=i,y0=CIlow[i], y1=CIup[i], col=select[i]+1)
}
```

As expected, 2 out of 20 CIs cover the respective parameters. 
However, if we select interesting parameters whose CIs do not cover 0, only 1 out of 3 CIs covers the selected parameters. 

### Inference after variable selection

The problem of variable selection arises when one wants to model the relationship between a variable of interest $Y$ and a subset of potential explanatory variables or predictors $X_1,\ldots,X_p$, but there is uncertainty about which subset to use. 

A variable selection algorithm can be used to find a subset $\hat{S}$. How to perform inference after variable selection?

Here we apply a variable selection algorithm to the Hitters data. We wish to predict a baseball player’s Salary on the basis of various covariates associated with performance in the previous year:

```{r}
library(ISLR)
Hitters =na.omit(Hitters)
covs=names(Hitters)[-19]
covs
set.seed(23)
Hitters$Salary = sample(Hitters$Salary)
```

We split the observations into a training set and a test set. 
We select the variables and perform inference on the selected model by using the training data: 

```{r}
set.seed(1)
train = sample(c(T,F), nrow(Hitters),rep=TRUE)
library(leaps)
fit_all = regsubsets(Salary ~ ., Hitters[train,])
fml = paste("Salary ~", paste(covs[summary(fit_all)$which[which.min(summary(fit_all)$cp),-1]], collapse=" + "))
round(
  summary(lm(fml, Hitters[train,]))$coeff
  ,4)
```

However, if we validate the previous inference on the test data we obtain:

```{r}
round(
  summary(lm(fml, Hitters[!train,]))$coeff
  ,4)
```

Statistical inference after variable selection requires an inference framework that takes the selection into account in order to be valid. 

# U.S. economy - politics 

You are a social scientist with a question: *Is the U.S. economy affected by whether Republicans or Democrats are in office?* 

Try to show that a connection exists, using real data going back to 1948. For your results to be publishable in an academic journal, you’ll need to prove that they are “statistically significant” by achieving a low enough $p$-value.

```{r}
rm(list=ls())
library(RCurl)
X <- read.csv("https://raw.githubusercontent.com/aldosolari/SL/master/data/X.csv", stringsAsFactors = FALSE)
Y <- read.csv("https://raw.githubusercontent.com/aldosolari/SL/master/data/Y.csv", stringsAsFactors = FALSE)
df = merge(X, Y, by.y = "date", by.x = "date", all=T)
df$date <- as.Date(df$date)
df2 = na.omit(df)
```

### Democrats perspective

How do you want to measure economic performance?

$\Box$ Employment
$\Box$ Inflation
$\boxtimes$ **GDP**
$\Box$ Stock prices

```{r}
plot(gdp ~ date, df2, type="l")
```

Which politicians do you want to include?

$\Box$ Presidents
$\boxtimes$ **Governors**
$\Box$ Senators
$\Box$ Representatives

```{r}
plot(gov_dem  ~ date, df, type="s")
```

Scatterplot gdp vs gov_dem:

```{r}
library(ggpubr)
ggscatter(df2, x = "gov_dem", y = "gdp",
          add = "reg.line", add.params = list(color = "blue"), xlab="More Democrats in Office", ylab="A Better Economy")
```

### Republicans perspective

How do you want to measure economic performance? 

$\boxtimes$ **Employment**
$\Box$ Inflation
$\Box$ GDP
$\Box$ Stock prices

```{r}
plot(unem ~ date, df, type="l")
```

Which politicians do you want to include?

$\Box$ Presidents
$\boxtimes$ **Governors**
$\Box$ Senators
$\Box$ Representatives

```{r}
plot(gov_gop ~ date, df, type="l")
```

Scatterplot unem vs gov_gop:

```{r}
library(ggpubr)
ggscatter(df2, x = "gov_gop", y = "unem",
          add = "reg.line", add.params = list(color = "red"), xlab="More Republicans in Office", ylab="A Better Economy")
```


# Jelly beans - acne

Let's imagine you are trying to find out if jelly beans cause acne. To do this you could find a group of people and randomly split them into two groups - one group who you get to eat lots of jelly beans and a second group who are banned from eating jelly beans. 

After some time you compare whether the group that eat jelly beans have more acne than those who do not. If more people in the group that eat jelly beans have acne then you might think that jelly beans cause acne. 

The scientists find no link between jelly beans and acne ($p$-value > 0.05) but then Megan and Cueball ask them to see if only one colour of jelly beans is responsible. They test 20 different colors each at a significance level of 5%, and they found a significant result for the green jelly beans. However, there is a problem.

![](https://imgs.xkcd.com/comics/significant.png)


# Dead salmon



# Error rates
